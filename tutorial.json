{"paragraphs":[{"text":"%md\n# Tutorial\n#### This is the tutorial demonstrating SparkML pipelines (https://spark.apache.org/docs/latest/ml-pipeline.html) based on housing prices Linear Regression done on the housing data set.","user":"anonymous","dateUpdated":"2018-04-14T10:57:45-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tutorial</h1>\n<h4>This is the tutorial demonstrating SparkML pipelines (<a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html\">https://spark.apache.org/docs/latest/ml-pipeline.html</a>) based on housing prices Linear Regression done on the housing data set.</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1523710573373_-1617764846","id":"20180414-085613_1002835270","dateCreated":"2018-04-14T08:56:13-0400","dateStarted":"2018-04-14T10:57:45-0400","dateFinished":"2018-04-14T10:57:49-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:224"},{"text":"%md\n#### First we will add the files we need to the hadoop file system.","user":"anonymous","dateUpdated":"2018-04-14T15:31:38-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>First we will add the files we need to the hadoop file system.</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1523712062437_1721968333","id":"20180414-092102_728660819","dateCreated":"2018-04-14T09:21:02-0400","dateStarted":"2018-04-14T15:31:38-0400","dateFinished":"2018-04-14T15:31:38-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:225"},{"text":"%md\n","user":"anonymous","dateUpdated":"2018-04-14T15:27:56-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523734076786_-1466728665","id":"20180414-152756_368860659","dateCreated":"2018-04-14T15:27:56-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:226"},{"text":"%md\n","user":"anonymous","dateUpdated":"2018-04-14T15:27:55-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523734075485_-1163546532","id":"20180414-152755_1262925587","dateCreated":"2018-04-14T15:27:55-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:227"},{"text":"%sh\nstart-all.sh\njps\n","user":"anonymous","dateUpdated":"2018-04-14T14:44:08-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","tableHide":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\n18/04/14 09:29:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /usr/local/hadoop-2.8.1/logs/hadoop-abhinav-namenode-abhinav-HP-Pavilion-x360-Convertible.out\nlocalhost: starting datanode, logging to /usr/local/hadoop-2.8.1/logs/hadoop-abhinav-datanode-abhinav-HP-Pavilion-x360-Convertible.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop-2.8.1/logs/hadoop-abhinav-secondarynamenode-abhinav-HP-Pavilion-x360-Convertible.out\n18/04/14 09:29:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nstarting yarn daemons\nstarting resourcemanager, logging to /usr/local/hadoop-2.8.1/logs/yarn-abhinav-resourcemanager-abhinav-HP-Pavilion-x360-Convertible.out\nlocalhost: starting nodemanager, logging to /usr/local/hadoop-2.8.1/logs/yarn-abhinav-nodemanager-abhinav-HP-Pavilion-x360-Convertible.out\n18/04/14 09:29:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/04/14 09:29:36 WARN ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: try once and fail.\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1373)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1337)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:787)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1700)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1433)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1685)\n\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:326)\n\tat org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:235)\n\tat org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:218)\n\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:165)\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:315)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:378)\nmkdir: Call From abhinav-HP-Pavilion-x360-Convertible/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n18/04/14 09:29:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/04/14 09:29:44 WARN ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: try once and fail.\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1373)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1337)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:787)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1700)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1433)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1685)\n\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:326)\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.getRemoteDestination(CommandWithDestination.java:195)\n\tat org.apache.hadoop.fs.shell.CopyCommands$Put.processOptions(CopyCommands.java:256)\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:164)\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:315)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:378)\nput: Call From abhinav-HP-Pavilion-x360-Convertible/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1523712348012_732223951","id":"20180414-092548_581084178","dateCreated":"2018-04-14T09:25:48-0400","dateStarted":"2018-04-14T09:28:59-0400","dateFinished":"2018-04-14T09:29:45-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:228"},{"text":"%md\n#### If you do not see namenode in the jps command (from the last command), please run the following block. Otherwise ignore it.\n","user":"anonymous","dateUpdated":"2018-04-14T09:32:18-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>If you do not see namenode in the jps command (from the last command), please run the following block. Otherwise ignore it.</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1523712667740_-1742235787","id":"20180414-093107_778652688","dateCreated":"2018-04-14T09:31:07-0400","dateStarted":"2018-04-14T09:32:18-0400","dateFinished":"2018-04-14T09:32:18-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:229"},{"text":"%sh\nstop-all.sh\nhdfs namenode -format\nstart-all.sh\njps\n","user":"anonymous","dateUpdated":"2018-04-14T14:44:36-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh\n"},{"type":"TEXT","data":"Paragraph received a SIGTERM\nExitValue: 143"}]},"apps":[],"jobName":"paragraph_1523712637515_-323143926","id":"20180414-093037_1755142570","dateCreated":"2018-04-14T09:30:37-0400","dateStarted":"2018-04-14T14:44:31-0400","dateFinished":"2018-04-14T14:44:41-0400","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:230"},{"text":"%sh\nhadoop -mkdir \nhadoop fs -put user /bin/files/train.csv \n","user":"anonymous","dateUpdated":"2018-04-14T11:51:02-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"18/04/14 09:55:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nput: `/bin/files/train.csv': No such file or directory: `hdfs://localhost:9000/bin/files/train.csv'\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1523712507128_488139907","id":"20180414-092827_15812022","dateCreated":"2018-04-14T09:28:27-0400","dateStarted":"2018-04-14T09:55:10-0400","dateFinished":"2018-04-14T09:55:14-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:231"},{"text":"%md\n#### In the next few blocks, we will run a simple linear regression model on a housing data set, aiming to predict sales price with quality. After reading in the dataset, we will cast the columns to doubles as the default schema specifies everything to strings. After this, we will create the objects for the various components. Due to the simplicity of this example, we mainly only have the vector assembler and the linear regression model. However, working with more complex models, the pipeline method allows much of the work to be abstracted","user":"anonymous","dateUpdated":"2018-04-14T12:30:37-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>In the next few blocks, we will run a simple linear regression model on a housing data set, aiming to predict sales price with quality. After reading in the dataset, we will cast the columns to doubles as the default schema specifies everything to strings. After this, we will create the objects for the various components. Due to the simplicity of this example, we mainly only have the vector assembler and the linear regression model. However, working with more complex models, the pipeline method allows much of the work to be abstracted</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1523722726402_-437379962","id":"20180414-121846_637944661","dateCreated":"2018-04-14T12:18:46-0400","dateStarted":"2018-04-14T12:30:22-0400","dateFinished":"2018-04-14T12:30:29-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:232"},{"text":"val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:////home/abhinav/Downloads/train.csv\")","user":"anonymous","dateUpdated":"2018-04-14T12:24:11-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.DataFrame = [Id: string, MSSubClass: string ... 79 more fields]\n"}]},"apps":[],"jobName":"paragraph_1523712920984_-1673782776","id":"20180414-093520_1660613904","dateCreated":"2018-04-14T09:35:20-0400","dateStarted":"2018-04-14T12:24:11-0400","dateFinished":"2018-04-14T12:24:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:233"},{"text":"df.printSchema()","user":"anonymous","dateUpdated":"2018-04-14T12:18:40-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Id: string (nullable = true)\n |-- MSSubClass: string (nullable = true)\n |-- MSZoning: string (nullable = true)\n |-- LotFrontage: string (nullable = true)\n |-- LotArea: string (nullable = true)\n |-- Street: string (nullable = true)\n |-- Alley: string (nullable = true)\n |-- LotShape: string (nullable = true)\n |-- LandContour: string (nullable = true)\n |-- Utilities: string (nullable = true)\n |-- LotConfig: string (nullable = true)\n |-- LandSlope: string (nullable = true)\n |-- Neighborhood: string (nullable = true)\n |-- Condition1: string (nullable = true)\n |-- Condition2: string (nullable = true)\n |-- BldgType: string (nullable = true)\n |-- HouseStyle: string (nullable = true)\n |-- OverallQual: string (nullable = true)\n |-- OverallCond: string (nullable = true)\n |-- YearBuilt: string (nullable = true)\n |-- YearRemodAdd: string (nullable = true)\n |-- RoofStyle: string (nullable = true)\n |-- RoofMatl: string (nullable = true)\n |-- Exterior1st: string (nullable = true)\n |-- Exterior2nd: string (nullable = true)\n |-- MasVnrType: string (nullable = true)\n |-- MasVnrArea: string (nullable = true)\n |-- ExterQual: string (nullable = true)\n |-- ExterCond: string (nullable = true)\n |-- Foundation: string (nullable = true)\n |-- BsmtQual: string (nullable = true)\n |-- BsmtCond: string (nullable = true)\n |-- BsmtExposure: string (nullable = true)\n |-- BsmtFinType1: string (nullable = true)\n |-- BsmtFinSF1: string (nullable = true)\n |-- BsmtFinType2: string (nullable = true)\n |-- BsmtFinSF2: string (nullable = true)\n |-- BsmtUnfSF: string (nullable = true)\n |-- TotalBsmtSF: string (nullable = true)\n |-- Heating: string (nullable = true)\n |-- HeatingQC: string (nullable = true)\n |-- CentralAir: string (nullable = true)\n |-- Electrical: string (nullable = true)\n |-- 1stFlrSF: string (nullable = true)\n |-- 2ndFlrSF: string (nullable = true)\n |-- LowQualFinSF: string (nullable = true)\n |-- GrLivArea: string (nullable = true)\n |-- BsmtFullBath: string (nullable = true)\n |-- BsmtHalfBath: string (nullable = true)\n |-- FullBath: string (nullable = true)\n |-- HalfBath: string (nullable = true)\n |-- BedroomAbvGr: string (nullable = true)\n |-- KitchenAbvGr: string (nullable = true)\n |-- KitchenQual: string (nullable = true)\n |-- TotRmsAbvGrd: string (nullable = true)\n |-- Functional: string (nullable = true)\n |-- Fireplaces: string (nullable = true)\n |-- FireplaceQu: string (nullable = true)\n |-- GarageType: string (nullable = true)\n |-- GarageYrBlt: string (nullable = true)\n |-- GarageFinish: string (nullable = true)\n |-- GarageCars: string (nullable = true)\n |-- GarageArea: string (nullable = true)\n |-- GarageQual: string (nullable = true)\n |-- GarageCond: string (nullable = true)\n |-- PavedDrive: string (nullable = true)\n |-- WoodDeckSF: string (nullable = true)\n |-- OpenPorchSF: string (nullable = true)\n |-- EnclosedPorch: string (nullable = true)\n |-- 3SsnPorch: string (nullable = true)\n |-- ScreenPorch: string (nullable = true)\n |-- PoolArea: string (nullable = true)\n |-- PoolQC: string (nullable = true)\n |-- Fence: string (nullable = true)\n |-- MiscFeature: string (nullable = true)\n |-- MiscVal: string (nullable = true)\n |-- MoSold: string (nullable = true)\n |-- YrSold: string (nullable = true)\n |-- SaleType: string (nullable = true)\n |-- SaleCondition: string (nullable = true)\n |-- SalePrice: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1523719903272_1893738647","id":"20180414-113143_411780148","dateCreated":"2018-04-14T11:31:43-0400","dateStarted":"2018-04-14T11:50:45-0400","dateFinished":"2018-04-14T11:50:45-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"text":"import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.CrossValidator\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.ParamGridBuilder\n","user":"anonymous","dateUpdated":"2018-04-14T12:54:44-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.CrossValidator\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.ParamGridBuilder\n"}]},"apps":[],"jobName":"paragraph_1523720073362_378290995","id":"20180414-113433_1394811231","dateCreated":"2018-04-14T11:34:33-0400","dateStarted":"2018-04-14T12:54:44-0400","dateFinished":"2018-04-14T12:54:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:235"},{"text":"val prelim_data  = df.withColumn(\"OverallQual\", $\"OverallQual\".cast(DoubleType))\n            .withColumn(\"OverallCond\", $\"OverallCond\".cast(DoubleType))\n            .withColumn(\"YearBuilt\", $\"YearBuilt\".cast(DoubleType))\n            .withColumn(\"SalePrice\", $\"YearBuilt\".cast(DoubleType))\n\n\n\nval assembler = new VectorAssembler()\n                .setInputCols(Array(\"OverallQual\",\"OverallCond\",\"YearBuilt\"))\n                .setOutputCol(\"Output\")\n\n                \nval Array(training, test) = data.randomSplit(Array(0.7, 0.3), seed = 45432)\n\n","user":"anonymous","dateUpdated":"2018-04-14T12:25:08-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"prelim_data: org.apache.spark.sql.DataFrame = [Id: string, MSSubClass: string ... 79 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_907bbf18caa2\ntraining: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Id: string, MSSubClass: string ... 80 more fields]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Id: string, MSSubClass: string ... 80 more fields]\n"}]},"apps":[],"jobName":"paragraph_1523712925841_-1020463819","id":"20180414-093525_1808006727","dateCreated":"2018-04-14T09:35:25-0400","dateStarted":"2018-04-14T12:25:08-0400","dateFinished":"2018-04-14T12:25:09-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236"},{"text":"val lr = new LinearRegression()\n        .setMaxIter(20)\n        .setRegParam(0.2)\n        .setElasticNetParam(0.5)\n        .setFeaturesCol(\"Output\")\n        .setLabelCol(\"SalePrice\")\n        \n","user":"anonymous","dateUpdated":"2018-04-14T12:25:09-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lr: org.apache.spark.ml.regression.LinearRegression = linReg_07c7771b8a2d\n"}]},"apps":[],"jobName":"paragraph_1523719179989_570389086","id":"20180414-111939_1448229292","dateCreated":"2018-04-14T11:19:39-0400","dateStarted":"2018-04-14T12:25:09-0400","dateFinished":"2018-04-14T12:25:10-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:237"},{"text":"val pipeline = new Pipeline()\n  .setStages(Array( assembler, lr))\n\nval trained_model = pipeline.fit(training)","user":"anonymous","dateUpdated":"2018-04-14T15:18:14-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pipeline: org.apache.spark.ml.Pipeline = pipeline_ec7f5a42773f\ntrained_model: org.apache.spark.ml.PipelineModel = pipeline_ec7f5a42773f\n"}]},"apps":[],"jobName":"paragraph_1523722887039_-1936748070","id":"20180414-122127_268323670","dateCreated":"2018-04-14T12:21:27-0400","dateStarted":"2018-04-14T12:25:11-0400","dateFinished":"2018-04-14T12:25:14-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:238"},{"text":"val predictions = trained_model.transform(test)\n\nval evaluation = new RegressionEvaluator()\n                .setLabelCol(\"SalePrice\")\n\n\nevaluation.isLargerBetter\n\nevaluation.evaluate(predictions)","user":"anonymous","dateUpdated":"2018-04-14T12:42:35-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"predictions: org.apache.spark.sql.DataFrame = [Id: string, MSSubClass: string ... 82 more fields]\nevaluation: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_0192dc8a5172\nres134: Boolean = false\nres136: Double = 0.2017305204637586\n"}]},"apps":[],"jobName":"paragraph_1523720262350_1777297648","id":"20180414-113742_1093627105","dateCreated":"2018-04-14T11:37:42-0400","dateStarted":"2018-04-14T12:42:35-0400","dateFinished":"2018-04-14T12:42:36-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:239"},{"text":"%md \nThe next few blocks will demonstrate the power of pipelines. Using the pipeline concept, we will optimize the hyperparameters for this model. Since, the model is somewhat simplistic, we will only work with the actual linear regression. In more complex applications, however, this can be extended to even the preprocessing states. For example, if a tokenizer is used, different types of splits or regular expression can be varied to see the best results. In this examples we will vary the max iterations, the regularization parameter, and the elastic net parameter. The following is a quick description of the parameters.\n\n*max iterations*: Pretty self-explanatory, this is the max iterations allowed by the model, which is related to the optimization function. In general, the error vs iterations graph is one of diminishing returns. Thus, spark terminates the optimization if the max iterations has not been reached but the error has been thorougly minimized.\n\n*regularization parameter*: regularization is used to prevent overfitting, which is when the model fits the training data but does not generalize well\n\n*elastic net parameter*: this represents the combination of two regularization methods: l1 (based on the vector distance) and l2 (based on the square of the vector distance). l2 is easier to optimize but l1 provides certain advantages. This is just the proportion that goes to l1, and therefore should be less than 1\n","user":"anonymous","dateUpdated":"2018-04-14T15:29:43-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The next few blocks will demonstrate the power of pipelines. Using the pipeline concept, we will optimize the hyperparameters for this model. Since, the model is somewhat simplistic, we will only work with the actual linear regression. In more complex applications, however, this can be extended to even the preprocessing states. For example, if a tokenizer is used, different types of splits or regular expression can be varied to see the best results. In this examples we will vary the max iterations, the regularization parameter, and the elastic net parameter. The following is a quick description of the parameters.</p>\n<p><em>max iterations</em>: Pretty self-explanatory, this is the max iterations allowed by the model, which is related to the optimization function. In general, the error vs iterations graph is one of diminishing returns. Thus, spark terminates the optimization if the max iterations has not been reached but the error has been thorougly minimized.</p>\n<p><em>regularization parameter</em>: regularization is used to prevent overfitting, which is when the model fits the training data but does not generalize well</p>\n<p><em>elastic net parameter</em>: this represents the combination of two regularization methods: l1 (based on the vector distance) and l2 (based on the square of the vector distance). l2 is easier to optimize but l1 provides certain advantages. This is just the proportion that goes to l1, and therefore should be less than 1</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1523726396022_672953631","id":"20180414-131956_814842721","dateCreated":"2018-04-14T13:19:56-0400","dateStarted":"2018-04-14T15:29:43-0400","dateFinished":"2018-04-14T15:29:43-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"text":"val lr2 = new LinearRegression()\n        .setRegParam(0.2)\n        .setElasticNetParam(0.5)\n        .setFeaturesCol(\"Output\")\n        .setLabelCol(\"SalePrice\")\n        \n        \nval pipeline2 = new Pipeline()\n  .setStages(Array(assembler, lr2))\n        \nval parameters = new ParamGridBuilder()\n                .addGrid(lr2.maxIter, Array(0,2,4,6,8,10,12,14,16))\n                .addGrid(lr2.regParam, Array(1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0))\n                .addGrid(lr2.elasticNetParam, Array(0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0))\n                .build()\n                \n                \nval cv = new CrossValidator()\n  .setEstimator(pipeline2)\n  .setEvaluator(evaluation)\n  .setEstimatorParamMaps(parameters)\n  .setNumFolds(2) \n","user":"anonymous","dateUpdated":"2018-04-14T13:01:51-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lr2: org.apache.spark.ml.regression.LinearRegression = linReg_4b8ac4442bc1\npipeline2: org.apache.spark.ml.Pipeline = pipeline_f0e20a685f60\nparameters: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tlinReg_4b8ac4442bc1-elasticNetParam: 0.05,\n\tlinReg_4b8ac4442bc1-maxIter: 0,\n\tlinReg_4b8ac4442bc1-regParam: 1.0\n}, {\n\tlinReg_4b8ac4442bc1-elasticNetParam: 0.1,\n\tlinReg_4b8ac4442bc1-maxIter: 0,\n\tlinReg_4b8ac4442bc1-regParam: 1.0\n}, {\n\tlinReg_4b8ac4442bc1-elasticNetParam: 0.15,\n\tlinReg_4b8ac4442bc1-maxIter: 0,\n\tlinReg_4b8ac4442bc1-regParam: 1.0\n}, {\n\tlinReg_4b8ac4442bc1-elasticNetParam: 0.2,\n\tlinReg_4b8ac4442bc1-maxIter: 0,\n\tlinReg_4b8ac4442bc1-regParam: 1.0\n}, {\n\tlinReg_4b8ac4442bc1-elasticNetParam: 0.25,\n\tlinReg_4b8ac4442bc1-maxIter: 0,\n\tlinReg_4b8ac4442bc1-regParam: 1.0\n}, {\n\tlinReg_4b8ac4442bc1-elasticNetParam: 0.3,\n\tlinReg_4b8ac4442bc1-maxIter: 0,\n\tlinReg_4b8ac4442bc1-regParam: 1.0\n}, {\n\tlinReg_4b8ac4442bc1-elasticNetPar...cv: org.apache.spark.ml.tuning.CrossValidator = cv_888620f72ba0\n"}]},"apps":[],"jobName":"paragraph_1523722635661_1519129492","id":"20180414-121715_1136861525","dateCreated":"2018-04-14T12:17:15-0400","dateStarted":"2018-04-14T13:01:51-0400","dateFinished":"2018-04-14T13:01:53-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:241"},{"text":"val hyper_model = cv.fit(training)","user":"anonymous","dateUpdated":"2018-04-14T13:01:58-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 9364 cancelled because SparkContext was shut down\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732)\n  at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n  at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651)\n  at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)\n  at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)\n  at org.apache.spark.SparkContext.stop(SparkContext.scala:1920)\n  at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)\n  at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n  at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)\n  at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n  at scala.util.Try$.apply(Try.scala:192)\n  at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n  at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n  at org.apache.spark.ml.regression.LinearRegressionSummary.numInstances$lzycompute(LinearRegression.scala:696)\n  at org.apache.spark.ml.regression.LinearRegressionSummary.numInstances(LinearRegression.scala:696)\n  at org.apache.spark.ml.regression.LinearRegressionSummary.<init>(LinearRegression.scala:701)\n  at org.apache.spark.ml.regression.LinearRegressionTrainingSummary.<init>(LinearRegression.scala:588)\n  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:225)\n  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:76)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:96)\n  at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)\n  at org.apache.spark.ml.Estimator$$anonfun$fit$1.apply(Estimator.scala:82)\n  at org.apache.spark.ml.Estimator$$anonfun$fit$1.apply(Estimator.scala:82)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n  at org.apache.spark.ml.Estimator.fit(Estimator.scala:82)\n  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:115)\n  at org.apache.spark.ml.tuning.CrossValidator$$anonfun$fit$1.apply(CrossValidator.scala:110)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:110)\n  ... 46 elided\n"}]},"apps":[],"jobName":"paragraph_1523724845887_1458768537","id":"20180414-125405_241858801","dateCreated":"2018-04-14T12:54:05-0400","dateStarted":"2018-04-14T13:01:58-0400","dateFinished":"2018-04-15T02:32:14-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"text":"hyper_model.transform(test)","user":"anonymous","dateUpdated":"2018-04-14T13:03:45-0400","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523725318100_83383164","id":"20180414-130158_210578679","dateCreated":"2018-04-14T13:01:58-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:243"}],"name":"tutorial","id":"2DC57Y4BM","angularObjects":{"2DBP6RJKQ:shared_process":[],"2DARKSNUA:shared_process":[],"2D94MX37X:shared_process":[],"2D85FCJJ5:shared_process":[],"2DBDEUF6E:shared_process":[],"2D8JEV5CF:shared_process":[],"2D9ETJ881:shared_process":[],"2D8JUDWCQ:shared_process":[],"2D9WU2SWS:shared_process":[],"2D7WRCEYW:shared_process":[],"2D8RJJTUE:shared_process":[],"2D86MG3KC:shared_process":[],"2D9K68X8Y:shared_process":[],"2D882RC9Q:shared_process":[],"2D929MTYN:shared_process":[],"2D9FG7TCC:shared_process":[],"2D9GD5URT:shared_process":[],"2D87DUYMW:shared_process":[],"2D9MRH73D:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}